# ðŸ‘ SHEEP AI Core

### Your AI forgets everything. SHEEP doesn't.

> Every AI conversation starts from zero. SHEEP gives AI agents persistent, causal memory â€” it remembers not just *what* happened, but *why*.

**SHEEP** (Sleep-based Hierarchical Emergent Entity Protocol) extracts facts and cause-effect relationships from natural conversation, consolidates them during sleep-like cycles, and recalls with reasoning. Not keyword matching. Not vector similarity. Actual understanding.

## Why SHEEP?

| Feature | ChatGPT Memory | Mem0 | Mastra OM | **SHEEP** |
|---------|---------------|------|-----------|-----------|
| Fact extraction | Basic | âœ… | âœ… | âœ… **95.7% F1** |
| Causal reasoning | âŒ | âŒ | âŒ | âœ… **100% F1** |
| Emotional memory | âŒ | âŒ | âŒ | âœ… **86% F1** |
| Sleep consolidation | âŒ | âŒ | âŒ | âœ… |
| Noise rejection | âŒ | ðŸŸ¡ | âœ… | âœ… **0 false positives** |
| GDPR compliance | âŒ | ðŸŸ¡ | âŒ | âœ… Built-in |
| Open source | âŒ | Partial | âœ… | âœ… MIT |

Ask ChatGPT "why did I switch to TypeScript?" and it draws a blank. Ask SHEEP and it returns: *"You switched because JavaScript had too many runtime bugs â†’ TypeScript compiler catches errors before production â†’ saved a week of debugging."* A full causal chain.

## Benchmarks

55 hand-labeled conversations. 152 expected facts. 41 causal links. Zero cherry-picking.

| Metric | Score |
|--------|-------|
| **Fact F1** | **95.7%** |
| **Causal F1** | **100%** |
| Fact Precision | 100% |
| Fact Recall | 87.5% |
| Recall Accuracy (end-to-end) | 85% |
| Emotional extraction F1 | 86% |
| False positives on small talk | **0** |

Run them yourself in 60 seconds:

```bash
npm run proof          # 5 cases, ~60s, ~$1
npm run proof:full     # 55 cases, ~12min
npm run proof:recall   # end-to-end pipeline test
```

## Install

```bash
npm install sheep-ai-core
```

## Quick Start

```typescript
import { SheepDatabase, extractFactsWithLLM, createSheepLLMProvider } from "sheep-ai-core";

// 1. Extract facts from any conversation
const llm = await createSheepLLMProvider("muscle");
const facts = await extractFactsWithLLM(llm, conversation, "episode-1");
// â†’ [{ subject: "user", predicate: "prefers", object: "TypeScript" }, ...]

// 2. Store in persistent memory
const db = new SheepDatabase("my-agent");
for (const fact of facts) db.insertFact(fact);

// 3. Query with causal reasoning
const chain = buildCausalChain(db.findCausalLinks({}), "switched to TypeScript");
// â†’ cause: "JavaScript runtime bugs" â†’ effect: "switched to TypeScript"
//   cause: "TypeScript compiler" â†’ effect: "saved a week of debugging"
```

## What SHEEP Extracts

**From a single conversation like:**
> "I'm so stressed about the release. The API keeps failing under load. I've been debugging for 12 hours."

**SHEEP extracts:**
- ðŸ“‹ Facts: `user | feeling | stressed`, `API | issue | failing under load`, `release | status | behind schedule`
- ðŸ”— Causal: `API failures` â†’ `stress and long debugging sessions`
- ðŸŽ­ Emotion: stressed (with cause and context)

**From noise like "Hey, nice weather today!"** â†’ SHEEP extracts **nothing**. Zero false positives.

## Architecture

```
Conversation â†’ [LLM Extraction] â†’ Facts + Causal Links + Episodes
                                        â†“
                              [Sleep Consolidation]
                                        â†“
                          Deduplicated, Connected Memory
                                        â†“
                              [Causal Recall Engine]
                                        â†“
                            "Why did X happen?" â†’ Chain
```

## License

MIT â€” use it however you want.

Built by [Marsirius AI Labs](https://github.com/mbmarsirius/SHEEP_CORE_v3.0.0)
